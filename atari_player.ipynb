{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "class Atari:\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "\n",
    "    def __init__(self, env_name, frame_height, frame_width, agent_history_length=4, no_op_steps=10):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        print(\"The environment has the following {} actions: {}\".format(self.env.action_space.n,\n",
    "                                                                        self.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "        # self.frame_processor = ProcessFrame()\n",
    "        self.current_state = np.empty((frame_height, frame_width, agent_history_length), dtype=np.uint8)\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.action_space_size = self.env.action_space.n\n",
    "        self.game_shape = self.env.observation_space.shape\n",
    "        self.is_graphical = True if len(self.game_shape) > 1 else False\n",
    "        self.frame_height = frame_height if self.is_graphical else self.game_shape[0]\n",
    "        self.frame_width = frame_width if self.is_graphical else 1\n",
    "\n",
    "    def reset(self, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            evaluation: A boolean saying whether the agent is evaluating or training\n",
    "        Resets the environment and stacks four frames ontop of each other to\n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True  # Set to true so that the agent starts\n",
    "        # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(np.random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1)  # Action 'Fire'\n",
    "        else:\n",
    "            frame, _, _, _ = self.env.step(1)\n",
    "\n",
    "        processed_frame = self.process(frame)\n",
    "        for i in range(self.agent_history_length):\n",
    "            self.update_current_state(processed_frame)\n",
    "\n",
    "        return terminal_life_lost\n",
    "\n",
    "    def process(self, frame):\n",
    "        # returns a (height, width, 1) array\n",
    "        if self.is_graphical:\n",
    "            # frame = frame[34:200, 0:160, :]\n",
    "            frame = resize(frame, (self.frame_height, self.frame_width))\n",
    "            frame = rgb2gray(frame)\n",
    "            frame = np.uint8(frame*255)\n",
    "        return frame\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: Integer, action the agent performs\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)  # (5â˜…)\n",
    "\n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "\n",
    "        processed_new_frame = self.process(new_frame)\n",
    "        self.update_current_state(processed_new_frame)\n",
    "\n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "\n",
    "    def get_current_state(self):\n",
    "        return self.current_state\n",
    "\n",
    "    def update_current_state(self, frame):\n",
    "        self.current_state = np.append(self.current_state[:, :, 1:], np.expand_dims(frame, axis=2), axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Lambda\n",
    "from keras.layers import Activation, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.initializers import VarianceScaling\n",
    "# from numba import *\n",
    "\n",
    "class QLearner:\n",
    "    def __init__(self, n_actions, learning_rate=0.00001,\n",
    "                 frame_height=84, frame_width=84, agent_history_length=4,\n",
    "                 batch_size=32, gamma=0.99, use_double_model=True):\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.use_double_model = use_double_model\n",
    "        # self.punishment = punishment\n",
    "\n",
    "        self.main_learner = DQN(self.n_actions, self.learning_rate,\n",
    "                                self.frame_height, self.frame_width, agent_history_length)\n",
    "\n",
    "        self.target_learner = DQN(self.n_actions, learning_rate,\n",
    "                                  self.frame_height, self.frame_width, agent_history_length)\n",
    "\n",
    "        self.targets = np.zeros((batch_size,))\n",
    "        self.set_computation_device()\n",
    "\n",
    "        # self.tbCallBack = [callbacks.TensorBoard(log_dir='./output/Tensorboards', histogram_freq=0, write_graph=True, write_images=True)]\n",
    "        self.tbCallBack = None\n",
    "\n",
    "    @staticmethod\n",
    "    def set_computation_device():\n",
    "        num_cores = 14\n",
    "        GPU = True\n",
    "\n",
    "        if GPU:\n",
    "            num_GPU = 1\n",
    "            num_CPU = 1\n",
    "        else:\n",
    "            num_CPU = 1\n",
    "            num_GPU = 0\n",
    "\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\n",
    "                                inter_op_parallelism_threads=num_cores,\n",
    "                                allow_soft_placement=True,\n",
    "                                device_count={'CPU': num_CPU,\n",
    "                                              'GPU': num_GPU}\n",
    "                                )\n",
    "\n",
    "        session = tf.Session(config=config)\n",
    "        K.set_session(session)\n",
    "\n",
    "    # @jit\n",
    "    def predict(self, states):\n",
    "        actions_mask = np.ones((states.shape[0], self.n_actions))\n",
    "        return self.main_learner.model.predict([states, actions_mask])  # separate old model to predict\n",
    "\n",
    "    # @jit\n",
    "    def train(self, current_state_batch, actions, rewards, next_state_batch, terminal_flags):\n",
    "\n",
    "        self.calculate_target_q_values(next_state_batch, terminal_flags, rewards)\n",
    "\n",
    "        one_hot_actions = np.eye(self.n_actions)[np.array(actions).reshape(-1)]\n",
    "        one_hot_targets = one_hot_actions * self.targets[:, None]\n",
    "\n",
    "        history = self.main_learner.model.fit([current_state_batch, one_hot_actions], one_hot_targets,\n",
    "                                 epochs=1, batch_size=self.batch_size, verbose=0, callbacks=self.tbCallBack)\n",
    "\n",
    "        return history.history['loss'][0]\n",
    "\n",
    "    # @jit\n",
    "    def update_target_network(self):\n",
    "        if self.use_double_model:\n",
    "            print('Updating the target network')\n",
    "            self.target_learner.model.set_weights(self.main_learner.model.get_weights())\n",
    "        else:\n",
    "            print('Doubling is off, no need to update target network')\n",
    "\n",
    "    # @jit\n",
    "    def calculate_target_q_values(self, next_state_batch, terminal_flags, rewards):\n",
    "        actions_mask = np.ones((self.batch_size, self.n_actions))\n",
    "        q_next_state = self.main_learner.model.predict([next_state_batch, actions_mask])  # separate old model to predict\n",
    "        action, _ = self.action_selection_policy(q_next_state)\n",
    "        if self.use_double_model:\n",
    "            q_target = self.target_learner.model.predict([next_state_batch, actions_mask])  # separate old model to predict\n",
    "        else:\n",
    "            q_target = q_next_state\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if terminal_flags[i]:\n",
    "                self.targets[i] = rewards[i]\n",
    "            else:\n",
    "                self.targets[i] = rewards[i] + self.gamma * q_target[i, action[i]]\n",
    "\n",
    "    # @jit\n",
    "    def action_selection_policy(self, q_values):\n",
    "        # v = q_values - q_values.min(axis=1).reshape((-1, 1))\n",
    "        # v += 1.0\n",
    "        # sums = v.sum(axis=1).reshape((-1, 1))\n",
    "        # v = v / sums\n",
    "        # v = np.cumsum(v, axis=1)\n",
    "        #\n",
    "        # res = np.empty(q_values.shape[0], dtype=np.int32)\n",
    "        # r = np.random.rand(q_values.shape[0])\n",
    "        # for i in range(q_values.shape[0]):\n",
    "        #     res[i] = np.argwhere(v[i,:] >= r[i])[0,0]\n",
    "\n",
    "        res = np.argmax(q_values, axis=1)\n",
    "        return res, q_values[0,res][0]\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    def __init__(self, n_actions, learning_rate=0.00001,\n",
    "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "        input_shape = (frame_height, frame_width, agent_history_length)\n",
    "        # model = self.legacy_model(input_shape, self.n_actions)\n",
    "        # model = self.dueling_convnet(input_shape, self.n_actions)\n",
    "        # model = self.my_convnet(input_shape, self.n_actions)\n",
    "        model = self.nature_convnet(input_shape, self.n_actions)\n",
    "        # model = self.small_nature_convnet(input_shape, self.n_actions)\n",
    "        # model = self.sim_nature_convnet(input_shape, self.n_actions)\n",
    "        # model = self.modular_convnet(input_shape, self.n_actions)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        optimizer = RMSprop(lr=self.learning_rate, rho=0.95)\n",
    "        # optimizer = Adam(lr=self.learning_rate)\n",
    "        model.compile(optimizer, loss=tf.losses.huber_loss)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def huber_loss(self, y, q_value):\n",
    "        error = K.abs(y - q_value)\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def legacy_model(input_shape, num_actions):\n",
    "        frames_input = layers.Input(input_shape, name='inputs')\n",
    "        actions_input = layers.Input((num_actions,), name='action_mask')\n",
    "\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "        conv_1 = layers.convolutional.Conv2D(\n",
    "            64, (8, 8), strides=(2, 2), activation='relu', kernel_initializer='VarianceScaling')(normalized)\n",
    "        conv_2 = layers.convolutional.Conv2D(\n",
    "            32, (4, 4), strides=(2, 2), activation='relu', kernel_initializer='VarianceScaling')(conv_1)\n",
    "        conv_3 = layers.convolutional.Conv2D(\n",
    "            32, (3, 3), strides=(1, 1), activation='relu', kernel_initializer='VarianceScaling')(conv_2)\n",
    "        conv_4 = layers.convolutional.Conv2D(\n",
    "            32, (7, 7), strides=(1, 1), activation='relu', kernel_initializer='VarianceScaling')(conv_3)\n",
    "\n",
    "        conv_flattened = layers.core.Flatten()(conv_4)\n",
    "        hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "        output = layers.Dense(num_actions)(hidden)\n",
    "\n",
    "        filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "        model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(input_shape, num_actions):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(\n",
    "            input_shape=input_shape))\n",
    "        model.add(Dense(\n",
    "            num_actions,\n",
    "            activation=None))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def convnet(input_shape, num_actions):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, 8, strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "        model.add(Conv2D(32, 4, strides=(2, 2), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(num_actions, activation=None))\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def convnet_bn(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(16, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(32, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(32, activation='relu')(net)\n",
    "        net = BatchNormalization()(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def simpler_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(16, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(32, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(32, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def nature_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(32, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(64, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Conv2D(64, 3, strides=(1, 1), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(512, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def small_nature_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(8, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(16, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Conv2D(16, 3, strides=(1, 1), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(512, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dueling_convnet(input_shape, num_actions):\n",
    "        initializer = VarianceScaling(scale=2.0)\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "        net = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(normalized)\n",
    "        net = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(net)\n",
    "        net = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(net)\n",
    "        net = Conv2D(1024, (7, 7), strides=(1, 1), activation='relu', kernel_initializer=initializer, padding='valid',\n",
    "                     use_bias=False)(net)\n",
    "\n",
    "        net = Flatten()(net)\n",
    "        advt = Dense(256, kernel_initializer=initializer)(net)\n",
    "        # advt = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        advt = Dense(num_actions)(advt)\n",
    "        value = Dense(256, kernel_initializer=initializer)(net)\n",
    "        # value = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        value = Dense(1)(value)\n",
    "        # now to combine the two streams\n",
    "        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis=-1, keep_dims=True))(advt)\n",
    "        value = Lambda(lambda value: tf.tile(value, [1, num_actions]))(value)\n",
    "        final = Add()([value, advt])\n",
    "\n",
    "        model = DQN.add_action_mask_layer(final, frames_input, num_actions)\n",
    "\n",
    "        # model = Model(inputs=inputs, outputs=final)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def my_convnet(input_shape, num_actions):\n",
    "        initializer = VarianceScaling(scale=2.0)\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "\n",
    "        net = Conv2D(32, (8, 8), strides=(4, 4),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(normalized)\n",
    "        net = Conv2D(64, (4, 4), strides=(2, 2),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "        net = Conv2D(64, (4, 4), strides=(1, 1),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "        net = Conv2D(64, (4, 4), strides=(1, 1),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "        net = Conv2D(128, (3, 3), strides=(1, 1),\n",
    "                     activation='relu', kernel_initializer=initializer,\n",
    "                     padding='valid', use_bias=False)(net)\n",
    "\n",
    "        net = Flatten()(net)\n",
    "        advt = Dense(32, kernel_initializer=initializer)(net)\n",
    "        # advt = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        advt = Dense(num_actions)(advt)\n",
    "        value = Dense(32, kernel_initializer=initializer)(net)\n",
    "        # value = Dense(50, kernel_initializer=initializer)(net)\n",
    "\n",
    "        value = Dense(1)(value)\n",
    "        # now to combine the two streams\n",
    "        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis=-1, keep_dims=True))(advt)\n",
    "        value = Lambda(lambda value: tf.tile(value, [1, num_actions]))(value)\n",
    "        final = Add()([value, advt])\n",
    "\n",
    "        model = DQN.add_action_mask_layer(final, frames_input, num_actions)\n",
    "\n",
    "        # model = Model(inputs=inputs, outputs=final)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def modular_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        # Vision\n",
    "        net = Conv2D(32, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        net = Conv2D(64, 4, strides=(2, 2), activation='relu')(net)\n",
    "        net = Conv2D(64, 3, strides=(1, 1), activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "\n",
    "        # Reasoning\n",
    "        net = Dense(256, activation='tanh')(net)\n",
    "        net = Dense(64, activation='tanh')(net)\n",
    "\n",
    "        # Action decision maker\n",
    "        net = Dense(32, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_nature_convnet(input_shape, num_actions):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        # net = Conv2D(32, 8, strides=(4, 4), activation='relu')(normalized)\n",
    "        # net = Conv2D(64, 4, strides=(2, 2), activation='relu')(net)\n",
    "        # net = Conv2D(64, 3, strides=(1, 1), activation='relu')(net)\n",
    "\n",
    "        net = Conv2D(64, 16, strides=(10, 10), activation='relu')(normalized)\n",
    "\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(512, activation='relu')(net)\n",
    "        net = Dense(num_actions, activation=None)(net)\n",
    "        model = DQN.add_action_mask_layer(net, frames_input, num_actions)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def add_action_mask_layer(final, frames_input, num_actions):\n",
    "        actions_input = layers.Input((num_actions,), name='action_mask')\n",
    "        filtered_output = layers.Multiply(name='QValue')([final, actions_input])\n",
    "        model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "# from memory import ReplayMemory\n",
    "# from learner import QLearner\n",
    "from keras.models import model_from_json\n",
    "# from numba import *\n",
    "\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, game_env, agent_history_length, total_memory_size, batch_size,\n",
    "                 learning_rate, init_epsilon, end_epsilon, minimum_observe_episode,\n",
    "                 update_target_frequency, train_frequency, gamma, exploratory_memory_size,\n",
    "                 punishment, reward_extrapolation_exponent, linear_exploration_exponent, use_double):\n",
    "\n",
    "        self.n_actions = game_env.action_space_size\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.epsilon = init_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.minimum_observe_episodes = minimum_observe_episode\n",
    "        self.update_target_frequency = update_target_frequency\n",
    "        self.game_env = game_env\n",
    "        self.train_frequency = train_frequency\n",
    "        self.exploratory_memory_size = exploratory_memory_size\n",
    "        self.linear_exploration_exponent = linear_exploration_exponent\n",
    "        self.use_double_model = use_double\n",
    "\n",
    "        if reward_extrapolation_exponent < 0:\n",
    "            use_estimated_reward = False\n",
    "        else:\n",
    "            use_estimated_reward = True\n",
    "\n",
    "        self.memory = ReplayMemory(self.game_env.frame_height, self.game_env.frame_width,\n",
    "                                   agent_history_length, total_memory_size,\n",
    "                                   batch_size, self.game_env.is_graphical,\n",
    "                                   punishment=punishment, use_estimated_reward=use_estimated_reward,\n",
    "                                   reward_extrapolation_exponent=reward_extrapolation_exponent,\n",
    "                                   linear_exploration_exponent=self.linear_exploration_exponent)\n",
    "\n",
    "        self.learner = QLearner(self.n_actions, learning_rate, self.game_env.frame_height, self.game_env.frame_width,\n",
    "                                agent_history_length, gamma=gamma, use_double_model=self.use_double_model)\n",
    "        self.losses = []\n",
    "        self.q_values = []\n",
    "\n",
    "        # self.actuator = ???\n",
    "\n",
    "    # @jit\n",
    "    def take_action(self, current_state, total_frames, evaluation=False):\n",
    "        if (np.random.rand() <= self.epsilon) or (total_frames < self.exploratory_memory_size) and (not evaluation):\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            current_state = np.expand_dims(current_state, axis=0)\n",
    "            q_values = self.learner.predict(current_state)\n",
    "\n",
    "            action, q_value = self.learner.action_selection_policy(q_values)\n",
    "            self.q_values.append(q_value)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # @jit\n",
    "    def learn(self, no_passed_frames):\n",
    "        if no_passed_frames % self.train_frequency == 0:\n",
    "            current_state_batch, actions, rewards, next_state_batch, terminal_flags = self.memory.get_minibatch()\n",
    "            loss = self.learner.train(current_state_batch, actions, rewards, next_state_batch, terminal_flags)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "        if no_passed_frames % self.update_target_frequency == 0:\n",
    "            self.learner.update_target_network()\n",
    "\n",
    "    # @jit\n",
    "    def updates(self, no_passed_frames, episode, action, processed_new_frame, reward, terminal_life_lost, episode_seq):\n",
    "        self.memory.add_experience(action, processed_new_frame, reward, terminal_life_lost, episode_seq, episode)\n",
    "\n",
    "        if no_passed_frames > self.exploratory_memory_size:\n",
    "            self.update_epsilon(episode)\n",
    "            self.learn(no_passed_frames)\n",
    "\n",
    "    # @jit\n",
    "    def update_epsilon(self, episode):\n",
    "        self.epsilon -= 0.00001\n",
    "        self.epsilon = max(self.epsilon, self.end_epsilon)\n",
    "        # print('Epsilon: ', str(self.epsilon))\n",
    "\n",
    "    # @jit\n",
    "    def save_player_learner(self, folder):\n",
    "        model_json = self.learner.main_learner.model.to_json(indent=4)\n",
    "        with open(''.join([folder, 'model_structure.jsn']), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        self.learner.main_learner.model.save_weights(''.join([folder, 'model_weights.wts']))\n",
    "\n",
    "    # @jit\n",
    "    def load_player_learner(self, folder):\n",
    "        json_file = open(''.join([folder, 'model_structure.jsn']), 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights(''.join([folder,'model_weights.wts']))\n",
    "\n",
    "        self.learner.main_learner.model = loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class HandleResults:\n",
    "\n",
    "    folder_to_use = ''\n",
    "    settings_file_name = 'settings.jsn'\n",
    "    time = datetime.datetime.now()\n",
    "\n",
    "    def __init__(self, game_env, out_folder):\n",
    "        # GAME_ENV = settings['GAME_ENV']\n",
    "        d = datetime.datetime.now().strftime(\"%y_%m_%d_%H_%M_%S\")\n",
    "        self.folder_to_use = ''.join([out_folder, game_env, '/results_', d, '/'])\n",
    "        os.makedirs(self.folder_to_use)\n",
    "        self.results_file_name = ''.join([self.folder_to_use, 'results.csv'])\n",
    "\n",
    "    def save_settings(self, settings, player):\n",
    "        settings_dict = settings\n",
    "\n",
    "        with open(''.join([self.folder_to_use, 'settings.jsn']), 'wt') as outfile:\n",
    "            json.dump(settings_dict, outfile, indent=4)\n",
    "\n",
    "        player.save_player_learner(self.folder_to_use)\n",
    "\n",
    "    def load_default_settings(self, GAME_ENV):\n",
    "\n",
    "        settings_dict = {}\n",
    "        file_name = './default_settings.jsn'  # default_settings.jsn i in the root\n",
    "        with open(file_name, 'rt') as json_file:\n",
    "            settings_dict = json.load(json_file)\n",
    "        settings_dict['GAME_ENV'] = GAME_ENV\n",
    "        game_env = Atari(settings_dict['GAME_ENV'], settings_dict['frame_height'], settings_dict['frame_width'],\n",
    "                         agent_history_length=settings_dict['AGENT_HISTORY_LENGTH'],\n",
    "                         no_op_steps=settings_dict['NO_OP_STEPS'])\n",
    "\n",
    "        player = self.build_player(settings_dict, game_env)\n",
    "\n",
    "        return player, game_env, settings_dict['MAX_EPISODE_LENGTH'], settings_dict['MAX_EPISODES'], settings_dict\n",
    "\n",
    "    def load_default_settings_constants(self, GAME_ENV):\n",
    "\n",
    "        settings_dict = {}\n",
    "#         file_name = './default_settings.jsn'  # default_settings.jsn i in the root\n",
    "#         with open(file_name, 'rt') as json_file:\n",
    "#             settings_dict = json.load(json_file)\n",
    "\n",
    "        settings_dict['GAME_ENV'] = GAME_ENV\n",
    "        settings_dict['AGENT_HISTORY_LENGTH'] = AGENT_HISTORY_LENGTH\n",
    "        settings_dict['MEMORY_SIZE'] = MEMORY_SIZE\n",
    "        settings_dict['BS'] = BS\n",
    "        settings_dict['LEARNING_RATE']=LEARNING_RATE\n",
    "        settings_dict['INI_EPSILON'] = INI_EPSILON\n",
    "        settings_dict['END_EPSILON'] = END_EPSILON\n",
    "        settings_dict['MIN_OBSERVE_EPISODE'] = MIN_OBSERVE_EPISODE\n",
    "        settings_dict['NETW_UPDATE_FREQ'] =NETW_UPDATE_FREQ\n",
    "        settings_dict['UPDATE_FREQ'] = UPDATE_FREQ\n",
    "        settings_dict['DISCOUNT_FACTOR'] = DISCOUNT_FACTOR\n",
    "        settings_dict['REPLAY_MEMORY_START_SIZE'] = REPLAY_MEMORY_START_SIZE\n",
    "        settings_dict['PUNISH'] = PUNISH\n",
    "        settings_dict['REWARD_EXTRAPOLATION_EXPONENT'] = REWARD_EXTRAPOLATION_EXPONENT\n",
    "        settings_dict['LINEAR_EXPLORATION_EXPONENT'] = LINEAR_EXPLORATION_EXPONENT\n",
    "        settings_dict['USE_DOUBLE_MODEL'] = USE_DOUBLE_MODEL\n",
    "        settings_dict['frame_height']= frame_height\n",
    "        settings_dict['frame_width']= frame_width\n",
    "        settings_dict['NO_OP_STEPS'] = NO_OP_STEPS\n",
    "        settings_dict['MAX_EPISODE_LENGTH'] = MAX_EPISODE_LENGTH\n",
    "        settings_dict['MAX_EPISODES'] = MAX_EPISODES\n",
    "\n",
    "        game_env = Atari(settings_dict['GAME_ENV'], settings_dict['frame_height'], settings_dict['frame_width'],\n",
    "                         agent_history_length=settings_dict['AGENT_HISTORY_LENGTH'],\n",
    "                         no_op_steps=settings_dict['NO_OP_STEPS'])\n",
    "\n",
    "        player = self.build_player(settings_dict, game_env)\n",
    "\n",
    "        return player, game_env, settings_dict['MAX_EPISODE_LENGTH'], settings_dict['MAX_EPISODES'], settings_dict\n",
    "\n",
    "    def load_settings(self, folder, load_model):\n",
    "        settings_dict = {}\n",
    "        with open(''.join([folder, self.settings_file_name]), 'rt') as json_file:\n",
    "            settings_dict = json.load(json_file)\n",
    "\n",
    "        game_env = Atari(settings_dict['GAME_ENV'], settings_dict['frame_height'], settings_dict['frame_width'],\n",
    "                         agent_history_length=settings_dict['AGENT_HISTORY_LENGTH'], no_op_steps=settings_dict['NO_OP_STEPS'])\n",
    "\n",
    "        player = self.build_player(settings_dict, game_env)\n",
    "\n",
    "        if load_model:\n",
    "            player.load_player_learner(folder)\n",
    "\n",
    "        return player, game_env, settings_dict['MAX_EPISODE_LENGTH'], settings_dict['MAX_EPISODES'], settings_dict\n",
    "\n",
    "    def save_res(self, res_dict):\n",
    "        if not os.path.isfile(self.results_file_name):\n",
    "            with open(self.results_file_name, mode='w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                headings = list(res_dict.keys())\n",
    "                writer.writerow(headings)\n",
    "            file.close()\n",
    "\n",
    "        with open(self.results_file_name, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            values = list(res_dict.values())\n",
    "            writer.writerow(values)\n",
    "        file.close()\n",
    "\n",
    "        print(res_dict)\n",
    "\n",
    "    def build_player(self, settings_dict, game_env):\n",
    "        player = Player(game_env, settings_dict['AGENT_HISTORY_LENGTH'], settings_dict['MEMORY_SIZE'],\n",
    "                        settings_dict['BS'],\n",
    "                        settings_dict['LEARNING_RATE'], settings_dict['INI_EPSILON'], settings_dict['END_EPSILON'],\n",
    "                        settings_dict['MIN_OBSERVE_EPISODE'], settings_dict['NETW_UPDATE_FREQ'],\n",
    "                        settings_dict['UPDATE_FREQ'], settings_dict['DISCOUNT_FACTOR'],\n",
    "                        settings_dict['REPLAY_MEMORY_START_SIZE'], settings_dict['PUNISH'],\n",
    "                        settings_dict['REWARD_EXTRAPOLATION_EXPONENT'], settings_dict['LINEAR_EXPLORATION_EXPONENT'],\n",
    "                        settings_dict['USE_DOUBLE_MODEL'])\n",
    "        return player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(max_episode_length, episode, game_env, player, total_frames, evaluation=False):\n",
    "    terminal_life_lost = game_env.reset()\n",
    "    episode_reward = 0\n",
    "    life_seq = 0\n",
    "    frame_number = 0\n",
    "    gif_frames = []\n",
    "    while True:\n",
    "        # Get state, make action, get next state (rewards, terminal, ...), record the experience, train if necessary\n",
    "        current_state = game_env.get_current_state()\n",
    "        action = player.take_action(current_state, total_frames, evaluation)\n",
    "        processed_new_frame, reward, terminal, terminal_life_lost, original_frame = game_env.step(action)\n",
    "\n",
    "        if frame_number >= max_episode_length:\n",
    "            terminal = True\n",
    "            terminal_life_lost = True\n",
    "\n",
    "        # if evaluation:\n",
    "        #     gif_frames.append(original_frame)\n",
    "\n",
    "        if not evaluation:\n",
    "            player.updates(total_frames, episode, action, processed_new_frame, reward, terminal_life_lost, life_seq)\n",
    "\n",
    "        episode_reward += reward\n",
    "        life_seq += 1\n",
    "\n",
    "        if terminal_life_lost:\n",
    "            life_seq = 0\n",
    "\n",
    "        # game_env.env.render()\n",
    "        total_frames += 1\n",
    "        frame_number += 1\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    return episode_reward, total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Flatten, Lambda\n",
    "from keras.layers import Activation, Input\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Add\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras import layers, callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "# from numba import *\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, frame_height, frame_width, agent_history_length=4, size=1000000, batch_size=32,\n",
    "                 is_graphical=True, use_spotlight=False, use_estimated_reward=True, punishment=0.0,\n",
    "                 reward_extrapolation_exponent=10.0, linear_exploration_exponent=True):\n",
    "\n",
    "        self.use_estimated_reward = use_estimated_reward\n",
    "        self.use_spotlight = use_spotlight\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        self.is_graphical = is_graphical\n",
    "        self.punishment_factor = punishment\n",
    "        self.reward_extrapolation_exponent = reward_extrapolation_exponent\n",
    "        self.linear_exploration_exponent = linear_exploration_exponent\n",
    "\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.backfill_factor = np.empty(self.size, dtype=np.float32)\n",
    "        self.backfilled_reward = np.empty(self.size, dtype=np.float32)\n",
    "\n",
    "        if is_graphical:\n",
    "            self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        else:\n",
    "            self.frames = np.empty((self.size, self.frame_height), dtype=np.float16)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        self.frame_number_in_epison = np.empty(self.size, dtype=np.int)\n",
    "        self.sparsity_lengths = []\n",
    "        self.min_reward = 0.0\n",
    "        self.max_reward = 0.0\n",
    "\n",
    "\n",
    "        if is_graphical:\n",
    "            self.minibatch_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "            self.minibatch_new_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                        self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        else:\n",
    "            self.minibatch_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                              self.frame_height), dtype=np.float16)\n",
    "            self.minibatch_new_states = np.empty((self.batch_size, self.agent_history_length,\n",
    "                                                  self.frame_height), dtype=np.float16)\n",
    "\n",
    "        self.minibatch_indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        self.minibatch_rewards = np.empty(self.batch_size, dtype=np.float32)\n",
    "\n",
    "        input_shape = (frame_height, frame_width, 1)\n",
    "\n",
    "        self.spotlight = SpotlightAttention(input_shape)\n",
    "\n",
    "    # @jit\n",
    "    def add_experience(self, action, frame, reward, terminal, frame_in_seq, episode):\n",
    "        self.min_reward = np.min((self.min_reward, reward))\n",
    "        self.max_reward = np.max((self.max_reward, reward))\n",
    "\n",
    "        if self.linear_exploration_exponent:\n",
    "            self.update_reward_exponent(episode)\n",
    "\n",
    "        if self.use_spotlight:\n",
    "            f = np.expand_dims(frame, axis=0)\n",
    "            f = np.expand_dims(f, axis=3)\n",
    "\n",
    "            seen_before = self.spotlight.seen_before(f)\n",
    "            self.spotlight.spotlight_train(f)\n",
    "        else:\n",
    "            seen_before = False\n",
    "\n",
    "        if not seen_before:\n",
    "            if terminal:\n",
    "                reward -= (self.punishment_factor*(self.max_reward + 1.0))\n",
    "                # reward -= self.punishment_factor\n",
    "\n",
    "            self.actions[self.current] = action\n",
    "            self.frames[self.current, ...] = frame\n",
    "            self.rewards[self.current] = reward\n",
    "            self.terminal_flags[self.current] = terminal\n",
    "            self.frame_number_in_epison[self.current] = frame_in_seq\n",
    "\n",
    "            if self.use_estimated_reward:\n",
    "                self.populate_reward_factors(reward)\n",
    "\n",
    "            self.count = max(self.count, self.current + 1)\n",
    "            self.current = (self.current + 1) % self.size\n",
    "\n",
    "    # @jit\n",
    "    def populate_reward_factors(self, current_reward):\n",
    "        if current_reward != 0:\n",
    "            prev_reward_indx = self.current - 1\n",
    "\n",
    "            while (self.frame_number_in_epison[prev_reward_indx] > 0) and (self.rewards[prev_reward_indx] == 0.0) \\\n",
    "                    and (prev_reward_indx > 0):\n",
    "                prev_reward_indx -= 1\n",
    "\n",
    "            start_indx = prev_reward_indx\n",
    "            end_indx = self.current\n",
    "            sparsity_length = end_indx - start_indx  # Length of consecutive zero rewards\n",
    "            self.sparsity_lengths.append(sparsity_length)\n",
    "            \n",
    "            self.backfilled_reward[end_indx] = current_reward\n",
    "            self.backfill_factor[end_indx] = 1.0\n",
    "\n",
    "            if sparsity_length < 15:\n",
    "                return\n",
    "            \n",
    "            for i in range(start_indx, end_indx):\n",
    "                self.backfill_factor[i] = (i - start_indx) / sparsity_length\n",
    "                self.backfilled_reward[i] = current_reward\n",
    "\n",
    "            \n",
    "    def update_reward_exponent(self, episode):\n",
    "        s_episode = START_EPISODE\n",
    "        e_episode = END_EPISODE\n",
    "        s_exponent = START_EXPONENT\n",
    "        e_exponent = END_EXPONENT\n",
    "\n",
    "        if episode < s_episode:\n",
    "            self.reward_extrapolation_exponent = s_exponent\n",
    "        if episode > e_episode:\n",
    "            self.reward_extrapolation_exponent = e_exponent\n",
    "        if (episode >= s_episode) and (episode <= e_episode):\n",
    "            self.reward_extrapolation_exponent = \\\n",
    "                ((e_exponent-s_exponent)/(e_episode-s_episode))*(episode-s_episode)+s_exponent\n",
    "\n",
    "        if e_episode > IGNORE_EXPONENT_EPISODE:\n",
    "            self.use_estimated_reward = False\n",
    "\n",
    "    # # @jit\n",
    "    # def get_estimated_reward(self, recent_reward, sparsity_length, current_index):\n",
    "    #     # return recent_reward*np.power(current_index/sparsity_length, self.reward_extrapolation_exponent)\n",
    "    #     return current_index / sparsity_length\n",
    "\n",
    "    # @jit\n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index - self.agent_history_length + 1:index + 1, ...]\n",
    "\n",
    "    # @jit\n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = np.random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                # if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                #     continue\n",
    "                if self.frame_number_in_epison[index] - self.frame_number_in_epison[index - self.agent_history_length] != self.agent_history_length:\n",
    "                    continue\n",
    "                break\n",
    "            self.minibatch_indices[i] = index\n",
    "\n",
    "    # @jit\n",
    "    def get_minibatch(self):\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "\n",
    "        self._get_valid_indices()\n",
    "\n",
    "        for i, idx in enumerate(self.minibatch_indices):\n",
    "            self.minibatch_states[i] = self._get_state(idx - 1)\n",
    "            self.minibatch_new_states[i] = self._get_state(idx)\n",
    "            if self.use_estimated_reward:\n",
    "                self.minibatch_rewards[i] = self.backfilled_reward[idx] * \\\n",
    "                                            np.power(self.backfill_factor[idx], self.reward_extrapolation_exponent)\n",
    "            else:\n",
    "                self.minibatch_rewards[i] = self.rewards[idx]\n",
    "\n",
    "        return np.transpose(self.minibatch_states, axes=(0, 2, 3, 1)), self.actions[self.minibatch_indices], \\\n",
    "               self.minibatch_rewards, np.transpose(self.minibatch_new_states, axes=(0, 2, 3, 1)), \\\n",
    "               self.terminal_flags[self.minibatch_indices]\n",
    "\n",
    "\n",
    "class SpotlightAttention:\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.embedding_dimension = 10\n",
    "        self.spotlight_model = self.build_spotlight_model(input_shape, self.embedding_dimension)\n",
    "        self.threshold = .01\n",
    "\n",
    "    def build_spotlight_model(self,input_shape, embedding_dimension):\n",
    "        frames_input = Input(shape=input_shape)\n",
    "        normalized = layers.Lambda(lambda x: x / 255.0, name='norm')(frames_input)\n",
    "        net = Conv2D(8, 8, strides=(4, 4), activation='relu', use_bias=False)(normalized)\n",
    "        net = Conv2D(16, 4, strides=(2, 2), activation='relu', use_bias=False)(net)\n",
    "        net = Conv2D(16, 3, strides=(1, 1), activation='relu', use_bias=False)(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(embedding_dimension * 2, activation='relu', use_bias=False)(net)\n",
    "        net = Dense(embedding_dimension, use_bias=False)(net)\n",
    "        model = Model(inputs=frames_input, outputs=net)\n",
    "        optimizer = RMSprop()\n",
    "        # optimizer = Adam(lr=self.learning_rate)\n",
    "        model.compile(optimizer, loss='mean_squared_error')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def spotlight_train(self, image):\n",
    "        out = np.ones((1,self.embedding_dimension))\n",
    "        history = self.spotlight_model.fit(image, out, epochs=1, verbose=0)\n",
    "\n",
    "    def seen_before(self, image):\n",
    "        res = self.spotlight_model.predict(image)\n",
    "        dist = np.linalg.norm(res-np.ones(self.embedding_dimension))\n",
    "        return dist < self.threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from player.player import Player\n",
    "# from environments.simulator import Atari\n",
    "import numpy as np\n",
    "import datetime\n",
    "# from utils import HandleResults\n",
    "import numba\n",
    "\n",
    "MAX_EPISODE_LENGTH= 18000\n",
    "NO_OP_STEPS= 10\n",
    "MAX_EPISODES= 2000\n",
    "AGENT_HISTORY_LENGTH= 4\n",
    "UPDATE_FREQ= 4\n",
    "NETW_UPDATE_FREQ= 10000\n",
    "REPLAY_MEMORY_START_SIZE = 50000\n",
    "DISCOUNT_FACTOR= 0.99\n",
    "MEMORY_SIZE = 1000000\n",
    "BS= 32\n",
    "LEARNING_RATE= 0.0001\n",
    "PUNISH= 1.0\n",
    "INI_EPSILON= 1.0\n",
    "END_EPSILON= 0.1\n",
    "MIN_OBSERVE_EPISODE= 200\n",
    "# GAME_ENV= \"BreakoutDeterministic-v4\"\n",
    "REWARD_EXTRAPOLATION_EXPONENT = 2.0\n",
    "frame_height = 84\n",
    "frame_width = 84\n",
    "LINEAR_EXPLORATION_EXPONENT = True\n",
    "USE_DOUBLE_MODEL = True\n",
    "\n",
    "START_EPISODE = 400\n",
    "END_EPISODE = 1800\n",
    "START_EXPONENT = 1.0\n",
    "END_EXPONENT = 40.0\n",
    "IGNORE_EXPONENT_EPISODE = 2000\n",
    "\n",
    "GAME_ENV = 'BreakoutDeterministic-v4'\n",
    "# GAME_ENV = 'BerzerkDeterministic-v4'\n",
    "# GAME_ENV = 'SpaceInvaders-v4' # 758 frames\n",
    "# GAME_ENV = 'Alien-v4' # 948 frames\n",
    "# GAME_ENV = 'Amidar-v4' # 812 frames\n",
    "# GAME_ENV = 'Venture-v4'\n",
    "# GAME_ENV = 'Assault-v4' # 876 frames\n",
    "# GAME_ENV = 'RoadRunner-v4' # 437 frames\n",
    "# GAME_ENV = 'PongDeterministic-v4'\n",
    "# GAME_ENV = 'AsterixDeterministic-v4'\n",
    "# GAME_ENV = 'MontezumaRevenge-v4'\n",
    "# GAME_ENV = 'ChopperCommand-v4'\n",
    "# OUT_FOLDER = './output/Punish_0_No_Reward_exploration/'\n",
    "# OUT_FOLDER = './output/Punish_1_No_Reward_exploration/'\n",
    "OUT_FOLDER = './output/Punish_1_Reward_exploration_2/'\n",
    "\n",
    "results_handler = HandleResults(GAME_ENV, OUT_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 9, 9, 64)     32832       conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 7, 7, 64)     36928       conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_35 (Flatten)            (None, 3136)         0           conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 512)          1606144     flatten_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 4)            2052        dense_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_70[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_36 (InputLayer)           (None, 84, 84, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "norm (Lambda)                   (None, 84, 84, 4)    0           input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 20, 20, 32)   8224        norm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 9, 9, 64)     32832       conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 7, 7, 64)     36928       conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_36 (Flatten)            (None, 3136)         0           conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_71 (Dense)                (None, 512)          1606144     flatten_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_72 (Dense)                (None, 4)            2052        dense_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "action_mask (InputLayer)        (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QValue (Multiply)               (None, 4)            0           dense_72[0][0]                   \n",
      "                                                                 action_mask[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "MAX_EPISODE_LENGTH :  18000\n",
      "NO_OP_STEPS :  10\n",
      "MAX_EPISODES :  2000\n",
      "AGENT_HISTORY_LENGTH :  4\n",
      "UPDATE_FREQ :  4\n",
      "NETW_UPDATE_FREQ :  10000\n",
      "REPLAY_MEMORY_START_SIZE :  50000\n",
      "DISCOUNT_FACTOR :  0.99\n",
      "MEMORY_SIZE :  1000000\n",
      "BS :  32\n",
      "LEARNING_RATE :  0.0001\n",
      "PUNISH :  1.0\n",
      "INI_EPSILON :  1.0\n",
      "END_EPSILON :  0.1\n",
      "MIN_OBSERVE_EPISODE :  200\n",
      "GAME_ENV :  BreakoutDeterministic-v4\n",
      "REWARD_EXTRAPOLATION_EXPONENT :  2.0\n",
      "LINEAR_EXPLORATION_EXPONENT :  True\n",
      "USE_DOUBLE_MODEL :  True\n",
      "frame_height :  84\n",
      "frame_width :  84\n",
      "****************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/clusterdata/uqmziaei/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '0:00:00.284216', 'episode': 0, 'total_frames': 132.0, 'epsilon': '1.000', 'highest_reward': 0, 'mean_rewards': 0.0, 'mean_loss': 'nan', 'estimating_reward': True, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:04.560309', 'episode': 10, 'total_frames': 2151.0, 'epsilon': '1.000', 'highest_reward': 4.0, 'mean_rewards': 1.8, 'mean_loss': 'nan', 'estimating_reward': True, 'reward_exponent': 1.0}\n",
      "{'time': '0:00:08.428763', 'episode': 20, 'total_frames': 3971.0, 'epsilon': '1.000', 'highest_reward': 4.0, 'mean_rewards': 1.4, 'mean_loss': 'nan', 'estimating_reward': True, 'reward_exponent': 1.0}\n"
     ]
    }
   ],
   "source": [
    "load_folder=''\n",
    "load_model=False\n",
    "\n",
    "# if load_folder is not '':\n",
    "#     player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "#         results_handler.load_settings(load_folder, load_model)\n",
    "# else:\n",
    "#     player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "#         results_handler.load_default_settings(GAME_ENV)\n",
    "\n",
    "player, game_env, max_episode_length, max_number_of_episodes, all_settings = \\\n",
    "    results_handler.load_default_settings_constants(GAME_ENV)\n",
    "\n",
    "    \n",
    "    \n",
    "for k, v in all_settings.items():\n",
    "    print(k, ': ', v)\n",
    "\n",
    "print('****************************')\n",
    "\n",
    "results_handler.save_settings(all_settings, player)\n",
    "\n",
    "\n",
    "res_dict = {}\n",
    "\n",
    "highest_reward = 0\n",
    "total_frames = 0.0\n",
    "prev_frames = 0.0\n",
    "all_rewards = []\n",
    "time = datetime.datetime.now()\n",
    "prev_time = time\n",
    "best_evaluation = 0\n",
    "\n",
    "for episode in range(max_number_of_episodes):\n",
    "    episode_reward, total_frames = run_episode(max_episode_length, episode, game_env, player, total_frames)\n",
    "\n",
    "    # all_rewards[episode] = episode_reward\n",
    "    all_rewards.append(episode_reward)\n",
    "\n",
    "    if episode_reward>highest_reward:\n",
    "        highest_reward = episode_reward\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        # evaluation_reward, _ = run_episode(max_episode_length, episode, game_env, player, 0, evaluation=True)\n",
    "\n",
    "        # if evaluation_reward > best_evaluation:\n",
    "        #     best_evaluation = evaluation_reward\n",
    "            # print('Best eval: ', str(best_evaluation))\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        res_dict['time'] = str(now - time)\n",
    "        res_dict['episode'] = episode\n",
    "        res_dict['total_frames'] = total_frames\n",
    "        res_dict['epsilon'] = format(player.epsilon, '.3f')\n",
    "        res_dict['highest_reward'] = highest_reward\n",
    "        # res_dict['best_eval'] = best_evaluation\n",
    "        res_dict['mean_rewards'] = np.mean(all_rewards[-10:])\n",
    "        res_dict['mean_loss'] = format(np.mean(player.losses[-10:]), '.5f')\n",
    "        # res_dict['memory_vol'] = player.memory.count\n",
    "        # res_dict['fps'] = (total_frames - prev_frames) / ((now - prev_time).total_seconds())\n",
    "        # res_dict['sparsity'] = np.mean(player.memory.sparsity_lengths[-10:])\n",
    "        res_dict['estimating_reward'] = player.memory.use_estimated_reward\n",
    "        res_dict['reward_exponent'] = player.memory.reward_extrapolation_exponent\n",
    "\n",
    "        results_handler.save_res(res_dict)\n",
    "\n",
    "        prev_time = now\n",
    "        prev_frames = total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3758337638216167728\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([42., 71.,  0., 49., 48.,  0., 40.,  0., 26., 37.,  0., 37.,  0.,\n",
       "        36., 30.,  0., 46.,  0., 51., 46.,  0., 28., 29.,  0., 23.,  0.,\n",
       "        14., 13.,  0., 18.,  0., 13., 15.,  0., 14.,  0., 23., 15.,  0.,\n",
       "         8., 16.,  0., 18.,  0., 12., 21.,  0., 15.,  0., 10.,  9.,  0.,\n",
       "        21.,  0., 19., 27.,  0., 18.,  0., 14., 17.,  0., 10., 12.,  0.,\n",
       "        18.,  0., 13., 19.,  0., 10.,  0., 14., 14.,  0., 20.,  0., 10.,\n",
       "        17.,  0., 25., 14.,  0., 19.,  0.,  9., 25.,  0., 20.,  0., 22.,\n",
       "        19.,  0., 19.,  0., 25., 20.,  0., 19.,  0., 22., 18.,  0., 14.,\n",
       "        10.,  0., 12.,  0., 19., 11.,  0., 12.,  0.,  8.,  9.,  0., 11.,\n",
       "         0.,  9.,  4.,  0.,  3.,  2.,  0.,  2.,  0.,  1.,  4.,  0.,  6.,\n",
       "         0.,  2.,  2.,  0.,  3.,  0.,  3.,  3.,  0.,  2.,  1.,  0.,  2.,\n",
       "         0.,  0.,  2.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  2.,\n",
       "         0.,  0.,  0.,  2.,  0.,  0.,  2.,  1.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.]),\n",
       " array([  1.  ,   1.61,   2.22,   2.83,   3.44,   4.05,   4.66,   5.27,\n",
       "          5.88,   6.49,   7.1 ,   7.71,   8.32,   8.93,   9.54,  10.15,\n",
       "         10.76,  11.37,  11.98,  12.59,  13.2 ,  13.81,  14.42,  15.03,\n",
       "         15.64,  16.25,  16.86,  17.47,  18.08,  18.69,  19.3 ,  19.91,\n",
       "         20.52,  21.13,  21.74,  22.35,  22.96,  23.57,  24.18,  24.79,\n",
       "         25.4 ,  26.01,  26.62,  27.23,  27.84,  28.45,  29.06,  29.67,\n",
       "         30.28,  30.89,  31.5 ,  32.11,  32.72,  33.33,  33.94,  34.55,\n",
       "         35.16,  35.77,  36.38,  36.99,  37.6 ,  38.21,  38.82,  39.43,\n",
       "         40.04,  40.65,  41.26,  41.87,  42.48,  43.09,  43.7 ,  44.31,\n",
       "         44.92,  45.53,  46.14,  46.75,  47.36,  47.97,  48.58,  49.19,\n",
       "         49.8 ,  50.41,  51.02,  51.63,  52.24,  52.85,  53.46,  54.07,\n",
       "         54.68,  55.29,  55.9 ,  56.51,  57.12,  57.73,  58.34,  58.95,\n",
       "         59.56,  60.17,  60.78,  61.39,  62.  ,  62.61,  63.22,  63.83,\n",
       "         64.44,  65.05,  65.66,  66.27,  66.88,  67.49,  68.1 ,  68.71,\n",
       "         69.32,  69.93,  70.54,  71.15,  71.76,  72.37,  72.98,  73.59,\n",
       "         74.2 ,  74.81,  75.42,  76.03,  76.64,  77.25,  77.86,  78.47,\n",
       "         79.08,  79.69,  80.3 ,  80.91,  81.52,  82.13,  82.74,  83.35,\n",
       "         83.96,  84.57,  85.18,  85.79,  86.4 ,  87.01,  87.62,  88.23,\n",
       "         88.84,  89.45,  90.06,  90.67,  91.28,  91.89,  92.5 ,  93.11,\n",
       "         93.72,  94.33,  94.94,  95.55,  96.16,  96.77,  97.38,  97.99,\n",
       "         98.6 ,  99.21,  99.82, 100.43, 101.04, 101.65, 102.26, 102.87,\n",
       "        103.48, 104.09, 104.7 , 105.31, 105.92, 106.53, 107.14, 107.75,\n",
       "        108.36, 108.97, 109.58, 110.19, 110.8 , 111.41, 112.02, 112.63,\n",
       "        113.24, 113.85, 114.46, 115.07, 115.68, 116.29, 116.9 , 117.51,\n",
       "        118.12, 118.73, 119.34, 119.95, 120.56, 121.17, 121.78, 122.39,\n",
       "        123.  ]),\n",
       " <a list of 200 Patch objects>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(player.memory.sparsity_lengths, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
